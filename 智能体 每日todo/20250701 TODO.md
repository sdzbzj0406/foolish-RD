
### **1. Transformer çš„ç»“æ„ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°é‡ï¼Œä»¥åŠé™¤ä»¥æ ¹å· dkd_k çš„åŸå› **

#### âœ… Transformer çš„ç»“æ„ï¼ˆä»¥æ ‡å‡† Encoder ä¸ºä¾‹ï¼‰ï¼š

- **æ¯å±‚åŒ…å«ä¸¤ä¸ªä¸»è¦å­å±‚**ï¼š
    
    1. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-head Self Attentionï¼‰**
        
    2. **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Networkï¼‰**
        
- **æ¯ä¸ªå­å±‚å‰å**éƒ½æœ‰ **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰** å’Œ **å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰**ï¼ˆä½ç½®å–å†³äºæ˜¯å¦æ˜¯ PreNormï¼‰ã€‚
    

#### âœ… å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°é‡ï¼š

å‡è®¾ï¼š

- è¾“å…¥ç»´åº¦ä¸º dmodeld_{\text{model}}
    
- å¤´æ•°ä¸º hh
    
- æ¯ä¸ªå¤´çš„ç»´åº¦ä¸º dk=dv=dmodel/hd_k = d_v = d_{\text{model}} / h
    

é‚£ä¹ˆå‚æ•°é‡åŒ…æ‹¬ï¼š

- **Query / Key / Value æŠ•å½±çŸ©é˜µ**ï¼šæ¯ä¸ªéƒ½æ˜¯ dmodelÃ—dkd_{\text{model}} \times d_kï¼Œæ€»å…± 3 ä¸ªï¼Œå…±ï¼š
    
    3Ã—dmodelÃ—dk=3Ã—dmodel2/h3 \times d_{\text{model}} \times d_k = 3 \times d_{\text{model}}^2 / h
- **è¾“å‡ºåˆå¹¶çš„çº¿æ€§å±‚**ï¼šç»´åº¦ hâ‹…dvâ†’dmodelh \cdot d_v \to d_{\text{model}}ï¼Œä¹Ÿæ˜¯ï¼š
    
    dmodelÃ—dmodeld_{\text{model}} \times d_{\text{model}}

ğŸ“Œ **æ€»å‚æ•°é‡ï¼š**

4Ã—dmodel24 \times d_{\text{model}}^2

ï¼ˆå¿½ç•¥ biasï¼‰

#### âœ… ä¸ºä»€ä¹ˆé™¤ä»¥ dk\sqrt{d_k}ï¼š

- **é˜²æ­¢å†…ç§¯è¿‡å¤§å¯¼è‡´ Softmax æ¢¯åº¦æ¶ˆå¤±æˆ–è¿‡å°–é”**ã€‚
    
- QKTQK^T æ˜¯ä¸€ç»„å‘é‡ç‚¹ç§¯ï¼ŒæœŸæœ›å€¼ä¸ç»´åº¦æˆæ­£æ¯”ï¼Œé™¤ä»¥ dk\sqrt{d_k} å¯ä»¥æ§åˆ¶å€¼çš„æ–¹å·®è¶‹äºç¨³å®šï¼Œä»è€Œä½¿ Softmax æ›´å¹³ç¨³ã€‚
    

---

### **2. RMSNorm vs LayerNorm çš„åŒºåˆ«**

|é¡¹ç›®|LayerNorm|RMSNorm|
|---|---|---|
|æ˜¯å¦å‡å‡å€¼|âœ… æ˜¯|âŒ å¦|
|å½’ä¸€åŒ–æ–¹å¼|xâˆ’mean(x)std(x)\frac{x - \text{mean}(x)}{\text{std}(x)}|xmean(x2)+Ïµ\frac{x}{\sqrt{\text{mean}(x^2) + \epsilon}}|
|å‚æ•°|æœ‰ scale å’Œ bias|é€šå¸¸åªæœ‰ scale|
|æ€§èƒ½|ç•¥æ…¢|æ›´å¿«ï¼Œé€‚ç”¨äºæ¨ç†ä¼˜åŒ–|
|ä½¿ç”¨åœºæ™¯|GPTã€BERT ç­‰ä¼ ç»Ÿæ¨¡å‹|GLM3ã€NormFormerã€Mamba ç­‰è½»é‡æ¨¡å‹æˆ–æ–°æ¶æ„|

ğŸ“Œ **æ€»ç»“**ï¼šRMSNorm æ˜¯ä¸€ç§æ›´è½»é‡çš„å½’ä¸€åŒ–æ–¹æ³•ï¼Œå‡å°‘äº†å‡å€¼è®¡ç®—ï¼Œè®¡ç®—æ›´å¿«ï¼Œæ•ˆæœç›¸å½“ã€‚

---

### **3. Pre-Norm vs Post-Normï¼šä¼˜åŠ£ä¸ä¸»æµæ¨¡å‹ä½¿ç”¨è¶‹åŠ¿**

|ç±»å‹|ç»“æ„ä½ç½®|ä¼˜ç‚¹|ç¼ºç‚¹|
|---|---|---|---|
|**Pre-Norm**|`Norm â†’ SubLayer â†’ Residual`|æ›´ç¨³å®šï¼Œæ˜“äºè®­ç»ƒï¼Œæ¢¯åº¦ä¼ æ’­é¡ºç•…|è®­ç»ƒåˆæœŸæ”¶æ•›ç¨æ…¢|
|**Post-Norm**|`SubLayer â†’ Residual â†’ Norm`|ç†è®ºæ›´æ¸…æ™°ï¼Œç¬¦åˆåŸå§‹ Transformer å®ç°|æ·±å±‚è®­ç»ƒä¸ç¨³å®šï¼Œå®¹æ˜“æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±|

ğŸ” **ä¸»æµæ¨¡å‹è¶‹åŠ¿**ï¼š

- **Pre-Norm** æ˜¯å½“å‰ä¸»æµï¼Œä¾‹å¦‚ï¼š
    
    - GPT-3ã€GPT-NeoXã€LLaMAã€PaLM ç­‰éƒ½ä½¿ç”¨ **Pre-Norm**
        
- Post-Norm å·²è¾ƒå°‘ä½¿ç”¨ï¼Œä»…æ—©æœŸ Transformer è®ºæ–‡ä¸­å¸¸è§
    

---

### **4. é™¤äº†å¤šå¤´æ³¨æ„åŠ›ï¼Œè¿˜æœ‰å“ªäº›æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›ï¼Ÿ**

ä»¥ä¸‹æ˜¯å‡ ä¸ªé‡è¦æ”¹è¿›ï¼š

|åç§°|è¯´æ˜|
|---|---|
|**Linear Attentionï¼ˆçº¿æ€§æ³¨æ„åŠ›ï¼‰**|å°†æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä» O(n2)O(n^2) é™åˆ° O(n)O(n)ï¼Œå¦‚ Performerã€Linformer|
|**Sparse Attention**|åªåœ¨å±€éƒ¨æˆ–é‡è¦ä½ç½®è®¡ç®—æ³¨æ„åŠ›ï¼Œä»£è¡¨å¦‚ Longformerã€BigBird|
|**Global Attention**|åœ¨å±€éƒ¨æ³¨æ„åŠ›ä¸­å¼•å…¥æŸäº› token çš„å…¨å±€æ„ŸçŸ¥èƒ½åŠ›ï¼ˆå¦‚ T5 çš„ Encoderï¼‰|
|**Talking Heads Attention**|åœ¨å¤šå¤´è¾“å‡ºä¹‹é—´åŠ å…¥éçº¿æ€§äº¤äº’å±‚ï¼Œæå‡å¤´ä¹‹é—´ååŒä½œç”¨|
|**Relative Positional Attention**|ä½ç½®ç¼–ç æ”¹è¿›ï¼Œå¦‚ Transformer-XLã€DeBERTa ä½¿ç”¨çš„ä½ç½®å·®ç¼–ç |
|**FlashAttention**|é’ˆå¯¹æ˜¾å­˜ç“¶é¢ˆçš„é«˜æ•ˆå®ç°ï¼Œä¿ç•™å…¨æ³¨æ„åŠ›ï¼Œæå‡è®¡ç®—æ•ˆç‡|

---

### **5. GQA ä¸ MoA çš„åŸç†åŠåŒºåˆ«**

#### âœ… GQAï¼ˆGrouped Query Attentionï¼‰

- å¼•å…¥èƒŒæ™¯ï¼šæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼ŒQ/K/Véƒ½æ˜¯ç‹¬ç«‹ä¸ºæ¯ä¸ªå¤´è®¡ç®—ï¼Œè®¡ç®—å’Œå‚æ•°é‡å¤§
    
- **GQA æ ¸å¿ƒæ€æƒ³**ï¼š
    
    - **å¤šä¸ª Query å¤´å…±äº«å°‘é‡çš„ Key/Value ç»„**
        
    - å³ï¼šç”¨æ›´å¤š Query å¤´ï¼ˆå¦‚ 64ï¼‰ï¼Œä½† Key/Value åªæœ‰è¾ƒå°‘ç»„ï¼ˆå¦‚ 8 ç»„ï¼‰
        

ğŸ“Œ **ä¼˜ç‚¹**ï¼š

- é™ä½å‚æ•°å’Œæ˜¾å­˜å ç”¨
    
- æå‡æ¨ç†é€Ÿåº¦
    
- åº”ç”¨äº LLaMA2 ç­‰æ¨¡å‹
    

#### âœ… MoAï¼ˆMixture of Attentionï¼‰

- ç±»ä¼¼äº MoEï¼ˆMixture of Expertsï¼‰æ€æƒ³
    
- å°†æ³¨æ„åŠ›å¤´åˆ†ç»„ï¼Œæ¯ç»„ä½œä¸ºä¸€ä¸ªâ€œä¸“å®¶â€ï¼Œä¸åŒ token æ¿€æ´»ä¸åŒä¸“å®¶ç»„ï¼ˆç¨€ç–æ¿€æ´»ï¼‰
    
- æ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›ä¸æ˜¯æ‰€æœ‰å¤´éƒ½å‚ä¸ï¼Œè€Œæ˜¯â€œè·¯ç”±â€åˆ°å°‘æ•°å‡ ç»„
    

ğŸ“Œ **ä¼˜ç‚¹**ï¼š

- å¤§å¹…æå‡æ¨¡å‹å®¹é‡ï¼ˆæ— éœ€çº¿æ€§å¢åŠ è®¡ç®—ï¼‰
    
- å…·å¤‡ç¨€ç–æ€§ã€å¯æ‰©å±•æ€§ï¼Œç±»ä¼¼ç¨€ç– MoE
    

### **6. DeepSeek-V2 çš„ MLAï¼ˆMixture of Long-term and Attentionï¼‰æœºåˆ¶è§£æ**

#### âœ… MLA æ˜¯ä»€ä¹ˆï¼Ÿ

MLA æ˜¯ DeepSeek-V2 å¼•å…¥çš„ä¸€ç§ **èåˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†èƒ½åŠ›çš„æ³¨æ„åŠ›æœºåˆ¶**ï¼Œå…¨ç§° **Mixture of Long-term and Attention**ã€‚

#### âœ… èƒŒæ™¯ï¼š

åœ¨æ ‡å‡† Transformer ä¸­ï¼Œ**è‡ªæ³¨æ„åŠ›åªèƒ½å¤„ç†æœ‰é™é•¿åº¦çš„ä¸Šä¸‹æ–‡**ï¼Œä¸èƒ½å¾ˆå¥½åœ°æ•æ‰é•¿æœŸä¾èµ–ã€‚MLA çš„è®¾è®¡ç›®æ ‡æ˜¯ç»“åˆï¼š

- **çŸ­æœŸå±€éƒ¨ä¿¡æ¯ï¼ˆé€šè¿‡æ™®é€š attentionï¼‰**
    
- **é•¿æœŸä¿¡æ¯è®°å¿†ï¼ˆé€šè¿‡ä¸€ä¸ªç¼“å­˜æˆ–å¤–éƒ¨è®°å¿†æ¨¡å—ï¼‰**
    

#### âœ… MLA çš„æ ¸å¿ƒæœºåˆ¶ï¼š

- **Query ä»ç„¶æ¥è‡ªå½“å‰ token**
    
- **Key / Value è¢«åˆ’åˆ†æˆä¸¤éƒ¨åˆ†**ï¼š
    
    - **Local Attention**ï¼šä¼ ç»Ÿçª—å£æ³¨æ„åŠ›ï¼Œç”¨äºæ•æ‰ä¸Šä¸‹æ–‡å±€éƒ¨ä¿¡æ¯
        
    - **Long-term Memory Attention**ï¼šæ¥è‡ªä¸€ä¸ªå¤–éƒ¨ memory cacheï¼ŒåŒ…å«å†å²å…³é”®ä¿¡æ¯ï¼ˆå¦‚å‰æ–‡ token æ€»ç»“ï¼‰
        
- **ç±»ä¼¼ Mixture of Experts çš„æ–¹å¼åŠ æƒä¸¤è€…è¾“å‡º**ï¼Œå¦‚ï¼š
    
    MLA(Q)=Î±â‹…Attentionlocal(Q,KL,VL)+(1âˆ’Î±)â‹…Attentionlong(Q,KG,VG)\text{MLA}(Q) = \alpha \cdot \text{Attention}_{\text{local}}(Q, K_{L}, V_{L}) + (1 - \alpha) \cdot \text{Attention}_{\text{long}}(Q, K_{G}, V_{G})
    
    å…¶ä¸­ Î±\alpha å¯èƒ½æ˜¯ learnable gateï¼Œä¹Ÿå¯èƒ½æ˜¯ä½ç½®ç›¸å…³å‡½æ•°ã€‚
    

#### âœ… å¥½å¤„ï¼š

- æ”¯æŒ **æé•¿ä¸Šä¸‹æ–‡**ï¼ˆç™¾ä¸‡ token çº§åˆ«ï¼‰
    
- æå‡é•¿æ–‡ç”Ÿæˆã€ä¸€è‡´æ€§å’Œé€»è¾‘æ¨ç†èƒ½åŠ›
    
- ç±»ä¼¼ RetNet/LongNet ç­‰æ–¹å‘ï¼Œä½†é›†æˆæ›´ä¼˜
    

---

### **7. å¤§æ¨¡å‹çš„åè®­ç»ƒæµç¨‹ï¼ˆPost-training Pipelineï¼‰**

å¤§æ¨¡å‹çš„è®­ç»ƒæµç¨‹ä¸€èˆ¬åˆ†ä¸º **ä¸‰ä¸ªé˜¶æ®µ**ï¼š

#### âœ… 1. é¢„è®­ç»ƒï¼ˆPretrainingï¼‰

- ç›®æ ‡ï¼šè®­ç»ƒè¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼ˆå¦‚ Causal LMï¼‰
    
- æ•°æ®ï¼šå¤§è§„æ¨¡é€šç”¨è¯­æ–™ï¼ˆç½‘é¡µã€ç™¾ç§‘ã€ä¹¦ç±ï¼‰
    
- æ–¹æ³•ï¼šæ— ç›‘ç£ï¼Œtoken-by-token é¢„æµ‹
    

#### âœ… 2. æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuning / SFTï¼‰

- ç›®æ ‡ï¼šè®©æ¨¡å‹å­¦ä¼šâ€œæŒ‰æŒ‡ä»¤åŠäº‹â€
    
- æ•°æ®ï¼š<æŒ‡ä»¤, è¾“å…¥, ç›®æ ‡è¾“å‡º> ä¸‰å…ƒç»„
    
- æ–¹æ³•ï¼šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuningï¼‰
    

#### âœ… 3. å¯¹é½è®­ç»ƒï¼ˆAlignmentï¼‰

- **3.1 Preference Optimizationï¼ˆå¦‚ DPOã€PPOï¼‰**ï¼š
    
    - æ”¶é›† A/B ç”¨æˆ·åå¥½ï¼Œä¼˜åŒ–æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½
        
- **3.2 RLHFï¼ˆå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼‰**ï¼š
    
    - ä½¿ç”¨å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ç”Ÿæˆ
        
- **3.3 Safety Finetuning / Filtering**ï¼š
    
    - è¿‡æ»¤è¾“å‡ºä¸å®‰å…¨å†…å®¹
        
- **3.4 Tool Use / Function Call**ï¼š
    
    - å¯¹é½æ¨¡å‹ç»“æ„è®©å…¶å­¦ä¼šè°ƒç”¨å¤–éƒ¨å·¥å…·
        

ğŸ“Œ æœ‰äº›æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Geminiï¼‰è¿˜ä¼šè¿›è¡Œ **Multimodal alignmentã€Multitask alignmentã€Distillation from expert agents ç­‰é«˜çº§é˜¶æ®µ**ã€‚

---

### 8. LoRA åŸç†ã€åˆå§‹åŒ–æ–¹å¼å’Œè®¾è®¡åŠ¨æœº**

#### âœ… LoRAï¼ˆLow-Rank Adaptationï¼‰åŸç†ï¼š

LoRA çš„ç›®æ ‡æ˜¯åœ¨ **å†»ç»“å¤§æ¨¡å‹æƒé‡çš„å‰æä¸‹ï¼Œåªè®­ç»ƒå°‘é‡æ–°å¢å‚æ•°**ï¼Œä»¥è¾¾åˆ°ä½æˆæœ¬å¾®è°ƒã€‚

å®ƒå‡è®¾æƒé‡çŸ©é˜µæ›´æ–° Î”W\Delta W æ˜¯ä¸€ä¸ªä½ç§©çŸ©é˜µï¼Œä½¿ç”¨å¦‚ä¸‹å½¢å¼æ›¿ä»£ç›´æ¥è®­ç»ƒï¼š

Wlora=W+Î”W=W+BAW_{\text{lora}} = W + \Delta W = W + BA

å…¶ä¸­ï¼š

- BâˆˆRdÃ—rB \in \mathbb{R}^{d \times r}
    
- AâˆˆRrÃ—kA \in \mathbb{R}^{r \times k}
    
- râ‰ªdr \ll dï¼Œé€šå¸¸æ˜¯ 4~64
    

LoRA å®é™…åšæ³•æ˜¯åœ¨æŸäº›ä½ç½®ï¼ˆå¦‚ attention çš„ Q/K/Vï¼‰ä¸Šæ·»åŠ ä¸€ä¸ª **å¯è®­ç»ƒçš„ä½ç§©åˆ†æ”¯**ï¼Œç„¶åè¾“å‡ºåŠ å’Œã€‚

---

#### âœ… å‚æ•°åˆå§‹åŒ–æ–¹å¼ï¼š

- é€šå¸¸ï¼š
    
    - AA ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼ˆå¦‚ 0 å‡å€¼ï¼Œè¾ƒå°æ–¹å·®ï¼‰
        
    - BB åˆå§‹åŒ–ä¸ºå…¨ 0 æˆ–å°æƒé‡å€¼
        
- å› ä¸ºï¼š
    
    - åˆå§‹æ—¶ BAâ‰ˆ0BA \approx 0ï¼Œä¸å½±å“é¢„è®­ç»ƒæ¨¡å‹æ¨ç†
        
    - è¿™æ ·ä¸ä¼šå¼•å…¥åˆæœŸæ€§èƒ½æŠ–åŠ¨æˆ–æ¢¯åº¦çˆ†ç‚¸
        

---

#### âœ… ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

- **ç†è®º**ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¤§å¤šæ•°æƒé‡å˜åŒ–å¯ä»¥è¢«å‹ç¼©æˆä½ç§©ç»“æ„ï¼ˆæœ‰è®ºæ–‡å®è¯ï¼‰
    
- **å®è·µ**ï¼šèŠ‚çœæ˜¾å­˜ã€è®­ç»ƒæ›´å¿«ã€å‚æ•°å°‘ï¼ˆç”šè‡³åƒå€å‡å°‘ï¼‰
    

---

### 9. é™¤äº† LoRAï¼Œè¿˜æœ‰å“ªäº›ä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆPEFTï¼‰ï¼Ÿ**

ä»¥ä¸‹æ˜¯å‡ ä¸ªå¸¸è§æ–¹æ¡ˆï¼š

|æ–¹æ³•|åŸç†|å‚æ•°å¢é‡|ç‰¹ç‚¹|
|---|---|---|---|
|**LoRA**|æ’å…¥ä½ç§©çŸ©é˜µ|æä½|æœ€å¸¸ç”¨ï¼Œæ¨ç†æ—¶å¯åˆå¹¶|
|**QLoRA**|LoRA + é‡åŒ–æ¨¡å‹ï¼ˆ4bitï¼‰|æä½|è¶…ä½èµ„æº GPU å¾®è°ƒæ–¹æ¡ˆ|
|**Adapter**|åœ¨ Transformer å±‚ä¹‹é—´æ’å°æ¨¡å—|ä¸­ç­‰|è®­ç»ƒä¸­æ’å…¥ï¼Œæ¨ç†æ—¶ä¸å½±å“ä¸»å¹²|
|**Prompt Tuning**|è®­ç»ƒå°‘é‡è™šæ‹Ÿ tokenï¼ˆå‰ç¼€ï¼‰|æä½|æœ€è½»é‡ï¼Œä½†é€‚é…èŒƒå›´æœ‰é™|
|**Prefix Tuning**|è®­ç»ƒ prefix vector æ³¨å…¥ KV|æä½|æ”¹ attention KVï¼Œä¿ç•™ä¸»æ¨¡å‹|
|**IAÂ³ / BitFit**|ä»…è®­ç»ƒæ³¨æ„åŠ›ç¼©æ”¾ç³»æ•°æˆ– bias|æä½|æ›´æé™å‚æ•°èŠ‚çœï¼Œæ€§èƒ½ç•¥é€Š|
|**MoRA / AdaLoRA**|LoRA æ”¹è¿›ç‰ˆæœ¬|æä½|è‡ªåŠ¨è°ƒ rank æˆ–ç¨€ç–é€‰æ‹©|
|**Diff Pruning**|ç¨€ç–è®­ç»ƒå·®å€¼å‚æ•°|æä½|æ§åˆ¶è®­ç»ƒçš„å‚æ•°å­é›†|

---

ğŸ“Œ **é€‰å‹å»ºè®®**ï¼š

- **ä½èµ„æº / å¤§æ¨¡å‹**ï¼šé¦–é€‰ **QLoRA**
    
- **å¤šä»»åŠ¡ / å¤šé¢†åŸŸè¿ç§»**ï¼š**Adapter** ç³»åˆ—
    
- **æè‡´å‚æ•°æ•ˆç‡**ï¼šPrefix / IAÂ³ / BitFit
    


### 10. å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ä¸ RLHF ä¸»æµç®—æ³•ï¼ŒPPO å’Œ DPO å±•å¼€è®²è®²ä¸€ä¸ª**

#### âœ… å¤§æ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹

1. **é¢„è®­ç»ƒï¼ˆPretrainingï¼‰**
    
    - æ•°æ®ï¼šæµ·é‡éç»“æ„åŒ–æ–‡æœ¬ï¼ˆç½‘é¡µã€ç™¾ç§‘ã€ä»£ç ç­‰ï¼‰
        
    - ç›®æ ‡ï¼šè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆCausal LMï¼‰
        
    - æŸå¤±å‡½æ•°ï¼šCrossEntropy Loss
        
2. **æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuning / SFTï¼‰**
    
    - æ•°æ®ï¼šäººå·¥æ„é€ æˆ–åˆæˆçš„æŒ‡ä»¤æ•°æ®ï¼ˆé—®ç­”/æ‘˜è¦/å¯¹è¯ç­‰ï¼‰
        
    - ç›®æ ‡ï¼šè®©æ¨¡å‹å­¦ä¼šæŒ‰ç”¨æˆ·æŒ‡ä»¤å®Œæˆä»»åŠ¡
        
3. **å¯¹é½è®­ç»ƒï¼ˆAlignmentï¼‰**
    
    - æ ¸å¿ƒç›®æ ‡ï¼šè®©æ¨¡å‹è¾“å‡ºæ›´åŠ ç¬¦åˆäººç±»åå¥½ã€å®‰å…¨ã€æœ‰ç”¨
        
    - æ–¹æ³•åŒ…æ‹¬ï¼š
        
        - **PPOï¼ˆProximal Policy Optimizationï¼‰**
            
        - **DPOï¼ˆDirect Preference Optimizationï¼‰**
            
        - **RLAIFï¼ˆReinforcement Learning from AI Feedbackï¼‰**
            

---

#### âœ… ä¸»æµ RLHF ç®—æ³•ï¼ˆé€‰æ‹©æ€§è®­ç»ƒï¼‰

|ç®—æ³•|ç‰¹ç‚¹|ä»£è¡¨æ¨¡å‹|
|---|---|---|
|**PPO**|å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¼•å…¥å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ trial-and-error å­¦ä¹ |InstructGPT, GPT-4|
|**DPO**|æ— éœ€è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œåªç”¨åå¥½æ•°æ®ç›´æ¥ä¼˜åŒ–åå¥½å‡½æ•°|LLaMA2ã€OpenChat|
|**SPIN / IPO / KTO**|DPO çš„åç»­å˜ä½“ï¼Œè¿›ä¸€æ­¥æ”¹è¿›ç¨³å®šæ€§|Claude, Zephyr|
|**RLAIF**|AI ç”Ÿæˆåå¥½è€Œä¸æ˜¯äººç±»æ ‡æ³¨ï¼ŒèŠ‚çœæˆæœ¬|OpenAssistant|

---

#### âœ… å±•å¼€è®²ï¼š**DPOï¼ˆDirect Preference Optimizationï¼‰**

- æ•°æ®æ¥æºï¼šâŸ¨query, chosen, rejectedâŸ© ä¸‰å…ƒç»„ï¼ˆäººç±»æˆ– AI åé¦ˆï¼‰
    
- ç›®æ ‡ï¼šä¼˜åŒ–æ¨¡å‹ä½¿å¾—å®ƒè¾“å‡º "chosen" çš„æ¦‚ç‡ > "rejected"
    

**æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥æœ€å°åŒ–å¦‚ä¸‹æŸå¤±ï¼ˆä¸éœ€è¦å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒæ¨¡æ‹Ÿï¼‰ï¼š

LDPO=âˆ’logâ¡(expâ¡(Î²sÎ¸(ychosen))expâ¡(Î²sÎ¸(ychosen))+expâ¡(Î²sÎ¸(yrejected)))\mathcal{L}_{\text{DPO}} = -\log \left( \frac{\exp(\beta s_\theta(y_{\text{chosen}}))}{\exp(\beta s_\theta(y_{\text{chosen}})) + \exp(\beta s_\theta(y_{\text{rejected}}))} \right)

- sÎ¸(y)s_\theta(y) æ˜¯æ¨¡å‹å¯¹è¾“å‡º y çš„æ‰“åˆ†ï¼ˆé€šå¸¸ä¸º log-probï¼‰
    
- Î² æ§åˆ¶è¾“å‡ºåå¥½å¼ºåº¦
    

**ä¼˜ç‚¹ï¼š**

- æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹
    
- ä¸ä¾èµ– RL æ¡†æ¶ï¼Œè®­ç»ƒç¨³å®š
    
- å’Œ SFT æŸå¤±å…¼å®¹ï¼Œå¯æ— ç¼æ•´åˆ
    

---

### 11. å¸¸ç”¨ä½ç½®ç¼–ç æœ‰å“ªäº›ï¼ŒRoPE ä¸ºä»€ä¹ˆå¤–æ¨æ€§å¥½ï¼Ÿ**

#### âœ… å¸¸è§ä½ç½®ç¼–ç æ–¹å¼ï¼š

|ç¼–ç æ–¹å¼|ç‰¹ç‚¹|ä¸¾ä¾‹|
|---|---|---|
|**ç»å¯¹ä½ç½®ç¼–ç ï¼ˆSinusoidalï¼‰**|ä¸å¯å­¦ä¹ ï¼Œå›ºå®šå‡½æ•°|åŸå§‹ Transformer|
|**å¯å­¦ä¹ ä½ç½®ç¼–ç **|éšå‚æ•°è®­ç»ƒå­¦ä¹ ä½ç½®ç‰¹å¾|BERTã€GPT-2|
|**ç›¸å¯¹ä½ç½®ç¼–ç **|è¡¨ç¤º token é—´è·ç¦»|Transformer-XL, DeBERTa|
|**RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**|é€šè¿‡å¤æ•°æ—‹è½¬å»ºæ¨¡ä½ç½®|GPTNeoX, LLaMA, ChatGLM|
|**ALiBiï¼ˆAttention with Linear Biasesï¼‰**|åœ¨ attention scores ä¸­ç›´æ¥åŠ çº¿æ€§åç½®|RWKV, Mamba|
|**NTK-Aware RoPE**|æ”¹è¿› RoPE çš„å¤–æ¨èƒ½åŠ›|GPT-4 ç›¸å…³æ¨æµ‹ç»“æ„|

---

#### âœ… RoPEï¼ˆRotary Positional Encodingï¼‰å¤–æ¨æ€§æ›´å¥½çš„åŸå› ï¼š

- å°†ä½ç½®ç¼–ç åµŒå…¥ **Q/K çš„æŠ•å½±ç©ºé—´**ï¼Œä½¿ç”¨ **äºŒç»´æ—‹è½¬çŸ©é˜µ**ï¼š
    
    RoPE(x,t)=xâ‹…R(t)\text{RoPE}(x, t) = x \cdot R(t)
    
    å…¶ä¸­ R(t)R(t) æ˜¯ä½ç½® t å¯¹åº”çš„æ—‹è½¬çŸ©é˜µ
    
- è¿™ç§æ–¹å¼å¯ä»¥è‡ªç„¶åœ°æ¨å¹¿åˆ°ä»»æ„ä½ç½®ï¼Œä¸ä¾èµ–äºå›ºå®šé•¿åº¦
    
- **å¤–æ¨æ€§å¼ºçš„åŸå› **ï¼š
    
    - ç¼–ç æ˜¯è¿ç»­å‡½æ•°ï¼Œå¯å¤–æ¨åˆ°é•¿ä½ç½®
        
    - åœ¨ attention ä¸­ä½ç½®å·®è¢«æ˜¾å¼å»ºæ¨¡ï¼Œä¸å­˜åœ¨ç¼–ç å†²çª
        
    - å’Œ token embedding è§£è€¦ï¼Œæ”¯æŒä¸Šç™¾ä¸‡ token é•¿åº¦ï¼ˆå¦‚ LLaMAï¼‰
        

---

### **12. å¤§æ¨¡å‹è®­ç»ƒä¸æ¨ç†ä¼˜åŒ–æ–¹æ¡ˆï¼šZeroã€æ··åˆç²¾åº¦ã€FlashAttention ç­‰**

#### âœ… è®­ç»ƒä¼˜åŒ–æ–¹æ¡ˆï¼š

|æ–¹æ³•|ä½œç”¨|æè¿°|
|---|---|---|
|**ZeROï¼ˆZero Redundancy Optimizerï¼‰**|å¹¶è¡Œä¼˜åŒ–|å°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€åˆ‡åˆ†åˆ°ä¸åŒ GPU ä¸Šï¼ŒèŠ‚çœå†…å­˜|
|**Mixed Precision Trainingï¼ˆFP16/BF16ï¼‰**|åŠ é€Ÿè®­ç»ƒ|å‡å°‘ç²¾åº¦ï¼Œå‡å°‘æ˜¾å­˜ã€æé«˜é€Ÿåº¦|
|**Activation Checkpointing**|çœå†…å­˜|ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘æ—¶é‡æ–°è®¡ç®—|
|**Gradient Accumulation**|å¤§ batch æ¨¡æ‹Ÿ|æ‹†åˆ†å¤§ batchï¼Œåœ¨å¤šæ­¥å†…ç´¯è®¡æ¢¯åº¦|

#### âœ… æ¨ç†ä¼˜åŒ–æ–¹æ¡ˆï¼š

|æ–¹æ³•|æè¿°|ä½œç”¨|
|---|---|---|
|**FlashAttention**|é«˜æ•ˆ Attention CUDA kernel|é™ä½æ˜¾å­˜ + æé«˜é€Ÿåº¦ï¼ˆçªç ´ softmax O(nÂ²) ç“¶é¢ˆï¼‰|
|**KV Cache**|ç¼“å­˜å†å² Key/Value|åŠ é€Ÿæ¨ç†ï¼Œé¿å…é‡å¤è®¡ç®—|
|**Quantizationï¼ˆINT8/4ï¼‰**|æ¨¡å‹å‹ç¼©|æé«˜æ¨ç†é€Ÿåº¦ï¼Œé™ä½æ˜¾å­˜å ç”¨|
|**Speculative Decoding**|å¤šå€™é€‰å¹¶è¡Œé¢„æµ‹|æå‡è§£ç æ•ˆç‡ï¼ˆç”¨äº GPT-4 ç­‰ï¼‰|

---

### **4. å¤§æ¨¡å‹å‚æ•°é‡æ€ä¹ˆä¼°ç®—ï¼Ÿä»¥ 7B ä¸ºä¾‹è¯´æ˜**

å¤§æ¨¡å‹çš„å‚æ•°ä¸»è¦æ¥è‡ªï¼š

- Attention æ¨¡å—ï¼ˆQ, K, V, è¾“å‡ºï¼‰
    
- Feed-Forward å±‚ï¼ˆé€šå¸¸ 4x éšè—ç»´åº¦ï¼‰
    
- Embedding å’Œ Output Head
    

#### âœ… å‡è®¾é…ç½®ï¼ˆLLaMA-7B ç±»ï¼‰ï¼š

|è¶…å‚æ•°|å€¼|
|---|---|
|å±‚æ•°|32|
|éšè—ç»´åº¦ï¼ˆd_modelï¼‰|4096|
|Attention å¤´æ•°|32|
|FFN éšå±‚ç»´åº¦|11008|
|è¯è¡¨å¤§å°|32K+|

#### âœ… ç²—ç•¥ä¼°ç®—ï¼š

- **Self-Attention**ï¼ˆæ¯å±‚ï¼‰ï¼š
    
    4Ã—dmodel2=4Ã—40962â‰ˆ67M4 \times d_{\text{model}}^2 = 4 \times 4096^2 \approx 67M
- **Feed Forward**ï¼ˆæ¯å±‚ï¼‰ï¼š
    
    2Ã—dmodelÃ—dffn=2Ã—4096Ã—11008â‰ˆ90M2 \times d_{\text{model}} \times d_{\text{ffn}} = 2 \times 4096 \times 11008 \approx 90M
- æ¯å±‚æ€»å‚æ•°çº¦ 150M Ã— 32 å±‚ â‰ˆ 4.8B
    
- Embedding + Output projection â‰ˆ 2B å·¦å³
    

ğŸ“Œ **æ€»å’Œ â‰ˆ 7B**

> è¯´æ˜ï¼šå®˜æ–¹æ¨¡å‹éƒ½ä¼šä½¿ç”¨ç²¾ç¡®ç»Ÿè®¡ï¼Œä¼°ç®—é€šå¸¸å¿½ç•¥ bias å’Œ LayerNorm

---

### **5. å¼€æ”¾é¢˜ï¼šå°æ•°æ®å¦‚ä½•è®­ç»ƒæ•ˆæœä¸é”™çš„å¤§æ¨¡å‹ï¼Ÿ**

è¿™æ˜¯ä¸€ä¸ªéå¸¸å®é™…åˆæœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå…³é”®ç­–ç•¥å¦‚ä¸‹ï¼š

---

#### âœ… ç­–ç•¥ 1ï¼šä½¿ç”¨ **é¢„è®­ç»ƒæ¨¡å‹ + å¾®è°ƒï¼ˆFinetuneï¼‰**

- ä» GPT/BERT/LLaMA ç­‰å¼€æ”¾é¢„è®­ç»ƒæ¨¡å‹å‡ºå‘
    
- åªç”¨å°‘é‡æ•°æ®åšå¾®è°ƒï¼ˆLoRA/Adapterï¼‰
    
- è°ƒæ•´æ–¹å¼ï¼š
    
    - å¢å¼ºæ•°æ®è´¨é‡ï¼ˆé«˜è´¨é‡é—®ç­”ã€ä»£ç ã€æ‘˜è¦ç­‰ï¼‰
        
    - ä½¿ç”¨ In-Context Learning ç²¾è°ƒ Prompt
        

---

#### âœ… ç­–ç•¥ 2ï¼šæ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰

- åˆ©ç”¨ **å·²æœ‰å¤§æ¨¡å‹è¾…åŠ©ç”Ÿæˆæ›´å¤šæ•°æ®**
    
- ç¤ºä¾‹ï¼š
    
    - ChatGPT è‡ªæˆ‘ç”Ÿæˆé—®ç­”
        
    - å¢åŠ  paraphrasingã€ç¿»è¯‘ã€æ‘˜è¦ç­‰å˜ç§
        
- åˆæˆåçš„æ•°æ®ç”¨äº SFTï¼ˆæ•ˆæœæ˜¾è‘—ï¼‰
    

---

#### âœ… ç­–ç•¥ 3ï¼šä»»åŠ¡å¼å¾®è°ƒ + å¤šä»»åŠ¡è”åˆè®­ç»ƒ

- å°†å¤šä¸ªå°ä»»åŠ¡åˆå¹¶ï¼ˆå¦‚æ‘˜è¦ + QA + ç¿»è¯‘ï¼‰
    
- è·¨é¢†åŸŸè®­ç»ƒå¯æå‡æ³›åŒ–èƒ½åŠ›
    
- é€‚åˆæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°æ›´å¥½
    

---

#### âœ… ç­–ç•¥ 4ï¼šé«˜æ•ˆå‚æ•°å¾®è°ƒï¼ˆPEFTï¼‰

- å¦‚ LoRAã€Prefix-Tuningã€Adapter
    
- ç”¨æå°‘çš„å‚æ•°å®ç°æ”¶æ•›ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ
    

---

#### âœ… ç­–ç•¥ 5ï¼šå¼ºåŒ– Prompt å·¥ç¨‹ + RAG

- ä¸è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯å¢å¼ºè°ƒç”¨æ–¹å¼
    
- ä½¿ç”¨ Retrieval-Augmented Generation ç»“åˆçŸ¥è¯†åº“
    
- é€‚åˆå°æ•°æ®åœºæ™¯å¿«é€Ÿæ­å»ºé«˜æ€§èƒ½ç³»ç»Ÿ
    

---

### âœ… æ€»ç»“

|é—®é¢˜|å…³é”®ç‚¹|
|---|---|
|å¤§æ¨¡å‹è®­ç»ƒæµç¨‹|é¢„è®­ç»ƒ â†’ SFT â†’ å¯¹é½ï¼ˆPPO/DPOï¼‰|
|DPO|æ— éœ€ RLï¼Œç›´æ¥ä¼˜åŒ–åå¥½ï¼Œè®­ç»ƒç¨³å®š|
|RoPE å¤–æ¨|æ—‹è½¬ç¼–ç åµŒå…¥ç©ºé—´ï¼Œè¿ç»­å‡½æ•°æ”¯æŒ extrapolation|
|è®­ç»ƒ/æ¨ç†ä¼˜åŒ–|ZeROã€æ··åˆç²¾åº¦ã€FlashAttentionã€KV cache|
|å‚æ•°ä¼°ç®—|åŸºäº d_modelã€å±‚æ•°ã€FFNã€Embedding ç»“æ„è®¡ç®—|
|å°æ•°æ®ç­–ç•¥|å¾®è°ƒ + æ•°æ®å¢å¼º + LoRA + RAG + Prompt å·¥ç¨‹|


## 1. **ã€ŠTowards AI Search Paradigmã€‹**ï¼ˆBaidu è«–æ–‡ï¼‰

Baidu æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘ä¸‹ä¸€ä»£â€œAI æœç´¢èŒƒå¼â€çš„è“å›¾ï¼Œæ­å»ºäº†ä¸€å¥—ç”±**å››ä¸ªè§’è‰²**ç»„æˆçš„å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼š**Masterã€Plannerã€Executorã€Writer** ([arxiv.org](https://arxiv.org/html/2506.17188v1?utm_source=chatgpt.com "Towards AI Search Paradigm - arXiv"))ã€‚

- **Master** åˆ¤æ–­æŸ¥è¯¢å¤æ‚åº¦ï¼Œå¹¶åŠ¨æ€è°ƒåº¦ä¸‹æ¸¸æ¨¡å—ï¼›
    
- **Planner** å°†ä»»åŠ¡åˆ†è§£ä¸º DAG å­ä»»åŠ¡ï¼Œå¹¶é€‰å–æœ€åˆé€‚å·¥å…·ï¼›
    
- **Executor** è°ƒç”¨å·¥å…·æ‰§è¡Œå­ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œæ£€ç´¢ä¸æ‰§è¡Œï¼›
    
- **Writer** ç»¼åˆç»“æœå¹¶ç”Ÿæˆæœ€ç»ˆå›å¤ï¼Œå¯¹é½ Threeâ€‘Hï¼ˆHelpfulnessã€Harmlessnessã€Honestyï¼‰æ ‡å‡†ï¼Œå¹¶ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿ ([arxiv.org](https://arxiv.org/html/2506.17188v1?utm_source=chatgpt.com "Towards AI Search Paradigm - arXiv"))ã€‚
    

æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¶µç›–äº†å·¥å…·æ£€ç´¢ã€åŸºç¡€è®¾æ–½ä¼˜åŒ–ã€å¯¹é½è®­ç»ƒæœºåˆ¶ã€å¯ä¿¡åº¦æ§åˆ¶ç­–ç•¥ç­‰ç³»ç»Ÿç»†èŠ‚ï¼Œå¹¶é€šè¿‡ A/B å®éªŒå±•ç¤ºï¼Œåœ¨å¤æ‚æŸ¥è¯¢ä¸Šç›¸æ¯”ä¼ ç»Ÿ Web Search å¯æå‡ 13% çš„ç”¨æˆ·æ»¡æ„åº¦æŒ‡æ ‡ ([arxiv.org](https://arxiv.org/html/2506.17188v1?utm_source=chatgpt.com "Towards AI Search Paradigm - arXiv"))ã€‚

---

## 2. **CLIP ä¸ LoRA åŸç†**

### ğŸ¯ CLIPï¼ˆContrastive Language-Image Pretrainingï¼‰

ç”± OpenAI æå‡ºï¼Œå°†å›¾åƒä¸æ–‡æœ¬ç¼–ç åˆ°å…±äº«å‘é‡ç©ºé—´ï¼Œä½¿ç”¨**å¯¹æ¯”å­¦ä¹ **ä½¿**æ­£æ ·æœ¬å¯¹**ï¼ˆå›¾æ–‡å¯¹åº”ï¼‰è·ç¦»é è¿‘ã€â€œè´Ÿæ ·æœ¬å¯¹â€è·ç¦»è¿œã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ï¼š

- å›¾åƒç¼–ç å™¨ï¼šå¦‚ ResNet æˆ– ViTï¼›
    
- æ–‡æœ¬ç¼–ç å™¨ï¼šTransformerï¼›
    
- å¯¹æ¯”æŸå¤±ï¼šå¯¹é½å›¾æ–‡è¯­ä¹‰ã€‚
    

### ğŸ¯ LoRAï¼ˆLow-Rank Adaptationï¼‰

æ˜¯ä¸€ç§**å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•**ï¼šåœ¨æ³¨æ„åŠ›æˆ– FFN çš„æƒé‡å˜æ¢ä¸­å¼•å…¥ä½ç§©å¢é‡ Î”W=BA\Delta W = BAï¼Œå…¶ä¸­çŸ©é˜µ A,BA, B çš„ç§©è¿œä½äºåŸç»´åº¦ã€‚

- åˆå§‹åŒ–ï¼šB=0B=0ï¼ŒAâˆ¼N(0,Îµ)A \sim \mathcal{N}(0, Îµ)ï¼Œä½¿åˆå§‹å¢é‡è¿‘ä¹ä¸º 0ï¼Œä¸æ‰°åŠ¨ä¸»æ¨¡å‹ï¼›
    
- è®­ç»ƒæ—¶åªæ›´æ–° A,BA, Bï¼Œæ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨ä¸è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™ä¸»å¹²æƒé‡ ([medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ï¼ˆLoRA æ–¹æ³•å¸¸è§ï¼‰ã€‚
    

---

## 3. **å¸¸è§å¤šæ¨¡æ€å¤§æ¨¡å‹ç®€ä»‹**

ä»¥ä¸‹æ˜¯å‡ æ¬¾å…·æœ‰ä»£è¡¨æ€§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼š

- **BLIP / BLIP-2 / BLIP-3 (xGenâ€‘MM)** ç³»åˆ— ([medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))
    
    - BLIP å¼•å…¥äº† **ITCï¼ˆContrastiveï¼‰+ ITMï¼ˆäºŒåˆ†ç±»åŒ¹é…ï¼‰+ CAPï¼ˆCaptioningï¼‰** ä¸‰é‡æŸå¤±ï¼Œæ­é… ViT + Transformer æ¶æ„ ([medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ã€‚
        
    - BLIPâ€‘2 å¢åŠ  Qâ€‘Formerï¼Œä»¥é¢„è®­ç»ƒå†»ç»“ç¼–ç å™¨ä¸ LLM ä¹‹é—´çš„â€œæ¡¥â€æœºåˆ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå¤§å¹…é™ä½è°ƒå‚é‡ ([medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ã€‚
        
    - BLIPâ€‘3ï¼ˆxGenâ€‘MMï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šæ›¿æ¢ Qâ€‘Former ä¸º perceiver resamplerï¼Œç»Ÿä¸€å•é˜¶æ®µè‡ªå›å½’ç›®æ ‡ï¼Œå¼ºåŒ–å¤§è§„æ¨¡è¯­æ–™ï¼Œå¹¶åŠ å…¥ DPO å¯¹é½ ([arxiv.org](https://arxiv.org/html/2408.08872v1?utm_source=chatgpt.com "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models - arXiv"))ã€‚
        
- **CLIP-empowered æ¨¡å‹**ï¼šå¦‚ Emu2, Seedâ€‘Xï¼Œä»¥åŠ BLIPâ€‘3 ä¸­ç”¨äºç”Ÿæˆéƒ¨åˆ†çš„ CLIP embedding + flow matching loss ([salesforce.com](https://www.salesforce.com/blog/blip3/?utm_source=chatgpt.com "BLIP3 - A Family of Fully Open Unified Multimodal Models - Salesforce"))ã€‚
    
- å…¶ä»–ï¼šFlamingoã€Geminiã€å¤šæ¨¡æ€ GPTã€Ernie 4.5 Turboï¼ˆBaiduï¼‰ç­‰ï¼Œä¹Ÿé‡‡ç”¨ç±»ä¼¼æ¶æ„ã€‚
    

---

## 4. **BLIP çš„ä¸‰ç§æŸå¤±åŠæ•°æ®æ¸…æ´—ç­–ç•¥**

### ğŸ”„ ä¸‰ç±»é¢„è®­ç»ƒæŸå¤±ï¼š

- **ITC (Imageâ€‘Text Contrastive)**ï¼šå¯¹é½å…¨å±€å›¾æ–‡åµŒå…¥ï¼›
    
- **ITM (Imageâ€‘Text Matching)**ï¼šå­¦ä¹ å›¾æ–‡æ˜¯å¦å¯¹åº”çš„äºŒåˆ†ç±»ï¼›
    
- **Captioning Loss (ç”Ÿæˆå¼/å›å½’)**ï¼šç”Ÿæˆå›¾åƒæè¿°ï¼Œä¼˜åŒ–è¯­è¨€ç”Ÿæˆä»»åŠ¡ ([ft.com](https://www.ft.com/content/c462fbd1-1672-4d8f-bd91-c3aa185d2418?utm_source=chatgpt.com "Baidu founder highlights 'shrinking' demand for DeepSeek's text-based AI"), [medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ã€‚
    

### ğŸ§¹ æ•°æ®æ¸…æ´—æµç¨‹ï¼š

- ä» COCOã€Visual Genomeã€Conceptual Captionsã€SBU ç­‰æä¾›çš„æ•°äº¿çº§åˆ«å›¾æ–‡å¯¹ä¸­ï¼Œé€šè¿‡â€œ**Bootstrap Captioning + Filtering**â€æœºåˆ¶ç”Ÿæˆå€™é€‰ï¼Œå†å‰”é™¤è´¨é‡ä¸é«˜çš„æ•°æ® ([medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ã€‚
    

---

## 5. **BLIPâ€‘2 vs BLIPï¼ŒåŠ BLIPâ€‘3 çš„æ”¹è¿›**

### â• BLIPâ€‘2 ç›¸æ¯” BLIP çš„æå‡ï¼š

- å¼•å…¥ **Qâ€‘Former**ï¼Œä½œä¸ºå†»ç»“å›¾åƒç¼–ç å™¨ä¸ LLM ä¹‹é—´çš„äº¤äº’æ¡¥æ¢ï¼›
    
- å›ºå®š ViT å’Œ LLMï¼Œä»…è®­ç»ƒå°é‡ Qâ€‘Formerï¼Œå¤§å¹…å‡å°è®­ç»ƒæˆæœ¬ï¼›
    
- ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼šå…ˆå›¾æ–‡ç†è§£ï¼Œå†å›¾æ–‡ç”Ÿæˆï¼Œå³å¯ä¿æŒä½æ•°ä¸‡å‚æ•°è°ƒä¼˜ ã€‚
    

### â• BLIPâ€‘3ï¼ˆxGenâ€‘MMï¼‰æ–°æ”¹è¿›ï¼š

- ä½¿ç”¨ **perceiver resampler** æ›¿ä»£ Qâ€‘Formerï¼Œæ¶æ„æ›´å¯æ‰©å±•ï¼›
    
- ç®€åŒ–æˆ **ç»Ÿä¸€çš„è‡ªå›å½’è®­ç»ƒæµç¨‹**ï¼Œä¸å†å¤šé˜¶æ®µï¼›
    
- å€ŸåŠ© DPO è¿›è¡Œå®‰å…¨å¯¹é½ï¼›
    
- å¤§è§„æ¨¡è¯­æ–™æ··è®­ï¼Œå¢å¼ºä¸Šä¸‹æ–‡ä¸å¤šæ¨¡èƒ½åŠ› ([arxiv.org](https://arxiv.org/html/2408.08872v1?utm_source=chatgpt.com "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models - arXiv"), [medium.com](https://medium.com/%40enrico.randellini/image-and-text-features-extraction-with-blip-and-blip-2-how-to-build-a-multimodal-search-engine-a4ceabf51fbe?utm_source=chatgpt.com "Image and text features extraction with BLIP and BLIP-2 - Medium"))ã€‚
    

---

## 6. **Qwenâ€‘VL çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹**

å…³äº Qwenâ€‘VLï¼ˆå¦‚ Qwenâ€‘VLâ€‘7B-Instruct ç­‰ï¼‰ï¼Œå¸¸è§ä¸‰é˜¶æ®µæµç¨‹åŒ…æ‹¬ï¼š

1. **è§†è§‰è¡¨å¾å¾®è°ƒï¼ˆVision Tuningï¼‰**ï¼šå†»ç»“æ–‡æœ¬æ¨¡å‹ä¸è§†è§‰ç¼–ç å™¨ï¼Œå¼•å…¥ Qâ€‘Former æˆ– prefixâ€‘tuning è‡ªé€‚åº”è§†è§‰ç‰¹å¾ï¼›
    
2. **è¯­è¨€å¯¹é½ï¼ˆInstructionâ€‘Tuningï¼‰**ï¼šåŠ å…¥å¤§è§„æ¨¡å¤šæ¨¡å¯¹è¯ã€é—®ç­”ã€ç±»åˆ«æ ‡æ³¨æ•°æ®è®­ç»ƒï¼›
    
3. **åå¥½æå‡ï¼ˆPreference Tuningï¼‰**ï¼šä½¿ç”¨ DPO æˆ– PPO ä¼˜åŒ–è¾“å‡ºåå¥½ã€å¯¹é½äººç±»åå¥½å’Œå®‰å…¨è¦æ±‚ã€‚
    

è¿™ä¸‰é˜¶æ®µåˆ†åˆ«å¯¹åº”â€œæŠ½å–è§†è§‰ä¿¡æ¯â†’ä½¿å…¶èƒ½æŒ‰æŒ‡ä»¤å¤šæ¨¡å¯¹è¯â†’å¯¹é½å®‰å…¨å¯æ§è¾“å‡ºâ€ã€‚


ä»¥ä¸‹æ˜¯ä½ æå‡ºå…­ä¸ªé—®é¢˜çš„è¯¦å®å›ç­”ï¼š

---

## 1ï¸âƒ£ è§†è§‰ç¼–ç å™¨ + LLM æ¥å…¥æ–¹å¼ï¼šBLIPâ€‘2 çš„ Qâ€‘Former vs LLaVA çš„ MLP

|æ–¹æ¡ˆ|ä¼˜ç‚¹|ç¼ºç‚¹|
|---|---|---|
|**Qâ€‘Former**ï¼ˆBLIPâ€‘2ï¼‰|â€¢ ç»“æ„åŒ–äº¤äº’ï¼šQuery-driven attentionï¼Œèƒ½ä¸»åŠ¨ç­›é€‰è¯­ä¹‰å…³é”®è§†è§‰ tokenâ€¢ ä¸é¢„è®­ç»ƒ LLM åŒ¹é…è‰¯å¥½ï¼Œé€‚é…å¤æ‚ä»»åŠ¡|â€¢ è¾ƒé‡ï¼šå¢åŠ å¤šå±‚ transformerï¼Œè®­ç»ƒæ¨ç†èµ„æºå¼€é”€å¤§|
|**ç®€å• MLP**ï¼ˆLLaVAï¼‰|â€¢ è½»é‡ç®€å•ï¼Œæ¨ç†æ•ˆç‡é«˜â€¢ æ˜“äºè®­ç»ƒï¼Œæ›´æ˜“éƒ¨ç½²|â€¢ è¡¨è¾¾èƒ½åŠ›å—é™ï¼Œä»…çº¿æ€§å˜æ¢ï¼Œéš¾æ•æ‰ä½ç½®æˆ–å…³ç³»ç»“æ„|

âœ¨ **ç»“è®º**ï¼šå¦‚éœ€ç²¾ç»†çš„è·¨æ¨¡æ€ç†è§£/ç”Ÿæˆä¸å¤æ‚æ¨ç†ï¼ŒQâ€‘Former æ›´åˆé€‚ï¼›è‹¥ç›®æ ‡ç®€æ´é«˜æ•ˆã€å¤šä¸ºæŒ‡ä»¤é—®ç­”ï¼ŒMLP å·²è¶³å¤Ÿã€‚

---

## 2ï¸âƒ£ Attention ä¸­é™¤ä»¥ âˆšdkï¼ˆåº”ä¸º âˆšdâ‚–ï¼‰

ä¸ºäº†è§£å†³ **ç‚¹ç§¯å€¼éšç»´åº¦å¢é•¿å¯¼è‡´ Softmax æ¢¯åº¦æ¶ˆå¤±æˆ–æç«¯é—®é¢˜**ï¼ŒTransformer åŸæ–‡å°†ï¼š

åšå½’ä¸€åŒ–å¤„ç†ï¼Œä»è€Œç¨³å®šè®­ç»ƒå’Œæå‡æ”¶æ•›é€Ÿåº¦ã€‚

---

## 3ï¸âƒ£ Qwen çš„ Transformer æ”¹è¿›ä¸ Qwen2 æ›´æ–°

### âœ… Qwen (é¦–ç‰ˆ)

- åŸºäº GPT æ¶æ„ï¼Œå¸¸è§„è‡ªå›å½’ Transformerï¼›
    
- ä½¿ç”¨ RMSNormã€Swiglu activationï¼›
    
- æ”¯æŒå¤š token å’Œç¼–ç å™¨-è§£ç å™¨å¾®è°ƒï¼›
    
- å‘å¸ƒå¤šä¸ªå‚æ•°é‡ç‰ˆæœ¬ï¼Œå¢åŠ ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ› ([en.wikipedia.org](https://en.wikipedia.org/wiki/Qwen?utm_source=chatgpt.com "Qwen"), [labellerr.com](https://www.labellerr.com/blog/dpo-vs-ppo-for-llm-all/?utm_source=chatgpt.com "DPO vs PPO: How To Align LLM [Updated] - Labellerr"), [unfoldai.com](https://unfoldai.com/qwen2-vl/?utm_source=chatgpt.com "Qwen2-VL â€” A new milestone in Vision-Language AI | UnfoldAI"), [reddit.com](https://www.reddit.com/r/machinelearningnews/comments/1icna8f/qwen_ai_releases_qwen25vl_a_powerful/?utm_source=chatgpt.com "Qwen AI Releases Qwen2.5-VL: A Powerful Vision-Language Model ..."))ã€‚
    

### âœ… Qwenâ€‘VL

- å¼•å…¥è§†è§‰ç¼–ç å™¨ + Qâ€‘Formerï¼›
    
- æ”¯æŒå›¾åƒç†è§£ + æŒ‡ä»¤å¾®è°ƒã€‚
    

### âœ… Qwen2â€‘VL åŠ Qwen2 æ›´æ–°

- æ”¯æŒåŠ¨æ€å›¾åƒåˆ†è¾¨ç‡ï¼›
    
- ç”¨ **Multimodal Rotary PEï¼ˆMâ€‘RoPEï¼‰** æ”¯æŒ 1D æ–‡æœ¬ã€2D å›¾åƒå’Œè§†é¢‘ï¼›
    
- å¼ºåŒ–è§†é¢‘ç†è§£ã€æ–‡æ¡£è§£æä¸å¤šè¯­è¨€æ”¯æŒ ([huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/qwen2_vl?utm_source=chatgpt.com "Qwen2-VL - Hugging Face"))ã€‚
    

---

## 4ï¸âƒ£ RHLFã€DPOã€PPO çš„åŒºåˆ«ã€Loss ä¸ä¼˜ç¼ºç‚¹

|æ–¹æ³•|ç±»å‹|Loss|ä¼˜ç‚¹|ç¼ºç‚¹|
|---|---|---|---|---|
|**PPO**|RLï¼ˆon-policy RLHFï¼‰|åŸºäº reward modelï¼Œå¸¦ä¼˜åŠ¿å‡½æ•° + clip + KL æ­£åˆ™â€ƒ([medium.com](https://medium.com/%40bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b?utm_source=chatgpt.com "RLHF(PPO) vs DPO. Although large-scale unsupervislyâ€¦ - Medium"))|éªŒè¯æˆç†Ÿã€å¯å¤„ç†å¤æ‚è¡Œä¸º|è®­ç»ƒä¸ç¨³å®šã€æ˜‚è´µã€éœ€ reward model|
|**DPO**|Offline preference learning||||
|([medium.com](https://medium.com/%40sulbha.jindal/policy-optimization-with-rlhf-ppo-dpo-orpo-d65d075d99f3?utm_source=chatgpt.com "Policy Optimization with RLHF â€” PPO/DPO/ORPO - Medium"))|ç®€å•ç¨³å®šï¼Œä¸è®­ç»ƒ reward modelï¼Œæ”¶æ•›å¿«|OOD é£é™©ã€å¯¹ Î² å’Œæ•°æ®æ•æ„Ÿ|||
|**RHLF**|RLHF é€šç”¨ç§°å‘¼|é€šå¸¸å³ PPO + äººç±»åé¦ˆ|å¼ºå¯¹é½èƒ½åŠ›|ä¸ PPO ç±»ä¼¼ï¼Œéœ€è¦å¤§é‡åé¦ˆ|

---

## 5ï¸âƒ£ å½“å‰å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸»è¦é—®é¢˜

- **é•¿ä¸Šä¸‹æ–‡ä¸é«˜åˆ†è¾¨ç‡ç†è§£ç“¶é¢ˆ**ï¼šè™½ç„¶ Mâ€‘RoPE/åŠ¨æ€åˆ†è¾¨ç‡æœ‰æ‰€æ”¹å–„ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œæ¨ç†æ…¢ï¼›
    
- **æ¨ç†æˆæœ¬é«˜**ï¼šè§†è§‰ + LLM + adaption compute å¤§ï¼›
    
- **å¯¹é½éš¾é¢˜**ï¼šè§†è§‰è¾“å‡ºæ­£ç¡®ä½†å¯èƒ½ä¸å®‰å…¨æˆ–è¯¯å¯¼ï¼›
    
- **å¤šæ¨¡æ€ååŒä¸è¶³**ï¼šå¤šå¹´åŒ–ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å¼±ï¼Œå¦‚è§†è§‰æ¨ç†ã€æ›´å¤æ‚è®°å¿†äº¤äº’ï¼›
    
- **æ•°æ®ä¸ç®—åŠ›éœ€æ±‚é«˜**ï¼šä¼˜è´¨å›¾æ–‡å¯¹å°‘ï¼Œè®­ç»ƒæˆæœ¬è´µã€‚
    

---

## 6ï¸âƒ£ å¤§æ¨¡å‹å‘å±•å†ç¨‹æ¦‚è§ˆ

1. **Transformerï¼ˆ2017ï¼‰**ï¼šAttention ä¸ºæ ¸å¿ƒï¼Œæ¨è¿› NLPï¼›
    
2. **BERTï¼ˆ2018ï¼‰**ï¼šåŒå‘ç¼–ç å™¨ + Maskï¼Œæ¨åŠ¨åˆ†ç±»ã€ç†è§£ï¼›
    
3. **GPTï¼ˆ2018~ï¼‰**ï¼šè‡ªå›å½’è§£ç å™¨ï¼Œå¼•å…¥ ChatGPT æµªæ½®ï¼›
    
4. **LLaMAï¼ˆ2023ï¼‰**ï¼šå‹ç¼©é«˜æ•ˆã€å¼€æ”¾",
    
5. **Qwenï¼ˆ2023~ï¼‰**ï¼šåŸºäº GPT ä¸”é›†æˆè§†è§‰ï¼ˆVL ç‰ˆæœ¬ï¼‰ã€æ”¯æŒå¤š tokenï¼›
    
6. **o1 ç±»æ¨¡å‹**ï¼ˆ2024~ï¼‰ï¼šæ›´å¤§ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ€ã€å°æ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬ï¼›
    
7. åˆ° **Qwen2â€‘VL / 2.5â€‘VLï¼ˆ2024/25ï¼‰**ï¼šèåˆåŠ¨æ€å›¾åƒå¤„ç†ã€é•¿è§†é¢‘ã€M-RoPEã€è§†è§‰ agent èƒ½åŠ› ([en.wikipedia.org](https://en.wikipedia.org/wiki/Qwen?utm_source=chatgpt.com "Qwen"), [reddit.com](https://www.reddit.com/r/machinelearningnews/comments/1icna8f/qwen_ai_releases_qwen25vl_a_powerful/?utm_source=chatgpt.com "Qwen AI Releases Qwen2.5-VL: A Powerful Vision-Language Model ..."))ã€‚
    

---

å¦‚ä½ å¯¹å…¶ä¸­æŸä¸ªç‚¹ï¼ˆå¦‚å…·ä½“ loss å®ç°ã€ä¾‹å­ã€ä»£ç ï¼‰éœ€è¦æ·±æŒ–ï¼Œæˆ–æƒ³çœ‹å›¾ç¤ºåˆ†æï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï½
1. ä»£ç ï¼šç¼–è¾‘è·ç¦»
2. ä»£ç ï¼šå®ç°å¤šå¤´è‡ªæ³¨æ„åŠ›
3. 6. ä»£ç ï¼š1143æœ€é•¿å…¬å…±å­åºåˆ—