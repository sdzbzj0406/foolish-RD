
# 1. ai项目中的embedding向量检索的原理是什么，如何保证检索的准确性
在 AI 项目中，**Embedding 向量检索（Vector Similarity Search）** 是信息检索、推荐系统、问答系统、智能体等核心组件之一。它的本质是：**将文本、图片、音频等复杂数据转化为稠密的向量表示，然后在向量空间中查找最相似的向量来进行匹配**。

---

### 一、Embedding 向量检索的原理

#### 1. 向量化表示（Embedding）

- 使用预训练的模型（如 `text-embedding-3-large`, `mxbai-embed-large`, `bge`, `E5`, etc.）将文本/图片/音频转换为一个高维向量（例如 768 维、1024 维）。
    
- 相似的语义内容会被映射到**相邻或接近**的向量。
    

#### 2. 存储向量

- 向量通常会存储在**向量数据库**中（如 FAISS、Milvus、Weaviate、Pinecone、Qdrant）。
    
- 向量可以配合元数据一起存储，用于增强过滤或后处理（如：所属文档、时间戳、标签等）。
    

#### 3. 相似度计算

- 检索时，将查询（query）也转为向量，然后与数据库中所有向量计算相似度（距离）。
    
- 常见的相似度计算方式：
    
    - **余弦相似度**：关注方向
        
    - **欧几里得距离（L2 距离）**：关注向量间的距离
        
    - **点积**：一般在向量已归一化时使用
        

#### 4. 返回 Top-K 相似向量

- 系统会根据相似度从高到低返回 Top-K（最相近的 K 个）向量，并附带原始内容或 ID 等。
    

---

### 二、如何保证检索的准确性

Embedding 检索的准确性 ≈ 表征能力 + 检索方式 + 下游处理

#### 1. 使用合适的 **Embedding 模型**

- 模型质量决定了向量表示的语义能力。
    
- 示例对比：
    
    - `text-embedding-3-large`（OpenAI）：适合英语、跨语种任务，效果稳定
        
    - `bge-large-zh`: 更适合中文语义
        
    - `E5` 模型：专注于信息检索（检索式问答）
        
- 可以根据任务 fine-tune 模型（如使用 DPO、Contrastive Learning）
    

#### 2. 向量预处理与归一化

- 通常使用 L2 归一化或单位向量，有利于余弦相似度计算的稳定性。
    
- 去除无效token，加入Prompt tuning、聚合策略等。
    

#### 3. 选择合适的 **相似度度量方式**

- 对于语义理解推荐使用余弦相似度；
    
- 在嵌入已归一化后可使用内积（点积）来加速。
    

#### 4. 合理的召回 + 精排策略（两阶段）

- 先用向量召回 Top-K → 再用 LLM 做 rerank。
    
- 示例：结合 Cohere Reranker、BGE-Rerank、ColBERT 等 rerank 模型。
    
- 可以提高查准率，同时过滤掉语义不清楚但向量接近的项。
    

#### 5. 搭配元信息增强检索（Hybrid Search）

- 向量匹配结合关键词/标签过滤（Hybrid Search）
    
- 例如：先筛选出某个知识库中的向量，再做向量 Top-K。
    

#### 6. 评估指标

- 精度指标：Recall@k、Precision@k、nDCG 等；
    
- 人工评估（准确率、回答是否可用）也很关键。
    

---

### 三、总结流程图（简化）

```
[原始内容]
     ↓  Embedding 模型
[向量表示: 768维]
     ↓ 存储
[向量数据库: FAISS/Milvus]
     ↓ 检索 query 向量
[Top-K 相似向量]
     ↓（可选 rerank / Filter）
[最终返回最相关内容]
```

---

### 四、典型应用场景

- 向量检索问答（RAG）
    
- 多轮对话历史记忆
    
- 推荐系统
    
- 图像搜索 / 视频片段搜索
    
- 代码检索 / API 匹配
    
- Agent 记忆、工具选择等
    

---

如你有具体项目场景（如 QA 系统、智能问答、知识库搜索等），我可以进一步给出选型建议和工程实践参考。是否需要深入到某个方向？
# 2.      function call如何解析用户的意图
解析用户意图并通过 function call 调用正确的函数，是现代 AI 智能体系统（如基于 OpenAI GPT 模型、LangChain、LangGraph 等）中的核心功能之一。这一过程通常被称为 **function call intent parsing（意图解析）**。

---

## Function Call 如何解析用户的意图？

可以理解为以下 **三个阶段**：

---

### 一、阶段 1：语义理解（用户意图识别）

**目标**：从自然语言中识别用户想做什么（即要调用哪个函数、传哪些参数）

#### LLM 的做法：

LLM 读取用户的 prompt（自然语言输入），并通过预训练学到的上下文理解能力来：

- 识别要调用的函数名（或工具名）
    
- 提取参数值（包括 string、数字、列表、对象等结构）
    
- 映射自然语言词语到结构化参数中
    

💡 **示例**

用户输入：

> 给我推荐三部2000年之后的科幻电影，评分在8分以上

LLM 理解为：

```json
{
  "function": "recommend_movies",
  "arguments": {
    "genre": "sci-fi",
    "min_rating": 8.0,
    "release_after": 2000,
    "limit": 3
  }
}
```

这个过程通过 function calling API（如 OpenAI 的 `function_call` / `tool_call`）、LangGraph 的 `ToolNode`、LangChain 的 `RunnableTool` 来触发。

---

### 二、阶段 2：函数定义约束与反向指导

为了提高准确性，系统通常会**提前注册所有可用的函数签名和参数说明**（类似于接口文档或 schema）：

```jsonc
{
  "name": "recommend_movies",
  "description": "根据用户偏好推荐电影",
  "parameters": {
    "type": "object",
    "properties": {
      "genre": {
        "type": "string",
        "description": "电影类型，例如 sci-fi, comedy"
      },
      "min_rating": {
        "type": "number",
        "description": "最低评分，例如 8.0"
      },
      "release_after": {
        "type": "integer",
        "description": "上映时间下限，例如 2000"
      },
      "limit": {
        "type": "integer",
        "description": "推荐条数"
      }
    },
    "required": ["genre"]
  }
}
```

LLM 会根据这些 schema 来“生成”结构化的参数输出。比如在 OpenAI GPT 中，它会输出：

```json
{
  "tool_calls": [
    {
      "function": "recommend_movies",
      "arguments": "{ \"genre\": \"sci-fi\", \"min_rating\": 8.0, \"release_after\": 2000, \"limit\": 3 }"
    }
  ]
}
```

这就是所谓的 **结构化意图解析**。

---

### 三、阶段 3：执行 & 结果反馈

- LLM 结构化地输出调用指令；
    
- 你在程序中解析这个 JSON 调用（如在 LangChain/LangGraph 中通过 `invoke()` 或 `yield` 调用）；
    
- 系统执行对应函数（数据库查找、API 查询、Agent 调用等）；
    
- 再把结果传回给 LLM，继续推理或生成回答。
    

---

## ✅ 准确解析意图的关键点

|影响因素|说明|
|---|---|
|🎯 函数描述是否清晰|描述越具体，模型越容易匹配用户意图|
|🧠 模型能力|GPT-4 系列对复杂结构化意图理解更好，GPT-3.5 有时会偏差较大|
|🧱 参数设计是否合理|如果参数定义太复杂或层级嵌套太深，模型提取容易出错|
|🧪 Prompt 工程|在系统 prompt 中加入示例（few-shot）、引导词（如“请使用以下工具回答”）可提升命中率|
|🔁 RAG 或检索增强|可借助外部知识库加强意图匹配，例如：先用向量检索找工具说明，再让 LLM 做 Function Call|

---

## ✅ 应用案例（LangChain / LangGraph / OpenAI）

- **LangChain**：定义 Tool（封装函数），让模型选择调用哪个
    
- **LangGraph**：在节点中注册工具，支持多轮调用
    
- **OpenAI function call**：注册函数 schema，GPT 负责解析、构造参数并输出调用请求
    

---

## ✅ 总结一句话：

> Function Call 是通过 LLM 理解用户输入语义后，**根据你提供的函数定义（schema）结构化地输出一个 API 调用指令**，这依赖模型的上下文理解能力、你对函数的描述设计、以及 prompt 工程优化。

---


# 3.      如何实现模型的对话记忆功能，比如使用队列，还是堆，原理是什么
要实现模型的「对话记忆功能」，本质是要让模型在多轮对话中能够“**记住并理解上下文**”，从而做出更连贯、有逻辑的回答。这种记忆能力在 RAG 系统、Agent 系统、智能客服、LLM 工作流（如 LangGraph）中都至关重要。

---

可以从两个层面来看：

### ✅ 1. **短期记忆（Short-Term Memory）**

就是当前对话的上下文，直接通过 prompt 压入模型上下文窗口中。

#### 实现方式：

- 使用 **队列（Queue）** 或 **列表（List）** 存储历史消息（`messages`）。
    
- 最近的消息在队尾，模型每次调用时都看到所有这些对话历史。
    
- 最终打包成一个 prompt，例如：
    

```json
[
  {"role": "user", "content": "你好"},
  {"role": "assistant", "content": "你好，请问有什么可以帮助的？"},
  {"role": "user", "content": "我想订一张机票"},
  ...
]
```

这个方案就是我们在 OpenAI Chat API 中常用的 `messages` 输入方式。

#### 容量限制：

- 模型有 **上下文长度限制**（如 GPT-4o 支持最多 128k tokens），当对话太长，需要裁剪或压缩旧消息。
    

---

### ✅ 2. **长期记忆（Long-Term Memory）**

长期记忆指的是**记住历史任务、用户习惯、知识点**，即使对话窗口已超出，也能记住关键信息。

#### 实现方式（典型做法）：

|技术方案|原理|是否类比堆/队列|
|---|---|---|
|✅ 向量数据库|将历史内容 embedding 存储，按需相似度召回|类似于“语义索引库”|
|✅ 结构化内存|以键值对或 JSON 存储信息（如用户名字、偏好等）|类似“命名变量表”|
|✅ RAG 记忆模块|对历史对话进行摘要、提取关键事件，再次嵌入使用|是“压缩后的摘要”|
|✅ LangGraph Memory|可配置短期（窗口队列）+ 长期（RAG 记忆）混合记忆|具备“队列 + 检索”的机制|

#### 类比结构说明：

|数据结构|类比说明|
|---|---|
|✅ **队列（Queue）**|适用于短期记忆：先进先出，保证最近上下文优先级高|
|✅ **堆（Heap）**|适用于记忆排序：比如按重要性排序、优先响应高权重事件（不常见）|
|✅ **HashMap / Dict**|结构化记忆，如记住“user_name” → “张杰”|
|✅ **向量索引库**|可类比“语义搜索型记忆”，不是结构顺序，而是语义相似度|

---

### ✅ 示例：LangChain / LangGraph 的记忆实现

#### ✅ LangChain 中的 Memory 类型：

```python
from langchain.memory import ConversationBufferMemory
```

|Memory 类型|特点|
|---|---|
|`ConversationBufferMemory`|类似队列，记录最近所有对话|
|`ConversationSummaryMemory`|自动生成对话摘要（更节省 token）|
|`ConversationBufferWindowMemory`|只保留最近 N 轮|
|`VectorStoreRetrieverMemory`|用向量召回历史记忆（长期记忆）|
|`CombinedMemory`|将多个记忆组合使用|

---

### ✅ 总结

> 🤖 **对话记忆的实现，通常是“短期记忆+长期记忆”结合：**

|类型|存储结构|实现方式|适用场景|
|---|---|---|---|
|短期记忆|队列/窗口缓冲区|把多轮对话拼成 prompt|连贯对话|
|长期记忆|向量库、键值对、结构化存储|用 embedding 检索或调用外部存储|用户画像、记住事实、Agent 记忆|
|记忆压缩|摘要、概括后再嵌入|防止 token 爆炸|长对话/多会话跨轮|

---

# 4.      文本怎么导入的向量知识库，切割的依据是什么
将文本导入向量知识库的过程是「**文本 → 切分 → Embedding → 存储（入库）**」。这通常是构建 **RAG（Retrieval-Augmented Generation）** 系统或 AI Agent 的第一步。

---

## ✅ 问题 4：文本怎么导入向量知识库，切割的依据是什么？

---

### 一、导入流程总览

```text
[原始文本/文档]
    ↓（文本切割：chunking）
[一段段内容片段]
    ↓（Embedding 模型）
[一堆向量]
    ↓（存入向量数据库）
[向量知识库完成]
```

---

### 二、切割的依据是什么？（Chunking 原理）

切割（chunking）是为了让每一段文本**既够长，有足够语义，又不超出模型长度限制**，方便后续 embedding 表示和检索。

#### 常见的切割策略：

|切割方式|切割依据|场景特点|
|---|---|---|
|✅ 固定字符数切割|如 500 字符一段|简单、适用于短文本|
|✅ 固定 token 数切割|如 256 tokens（考虑模型 token 限制）|GPT 相关模型更适合|
|✅ 基于段落/句子边界|每段内容完整断句切割|可保留语义完整性，适合文档|
|✅ 结合语义分割（TextSplitter）|语言分析+段落规则|避免切在句中，提高召回效果|
|✅ 自定义 Markdown / HTML 节点分割|按标题、列表、表格等结构分|适合结构化文本，如知识库、网页|

#### 示例（LangChain 切割工具）：

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # 每段最大字符数
    chunk_overlap=100,       # 重叠字符数，防止语义断裂
)
docs = text_splitter.create_documents([raw_text])
```

📌 **chunk_overlap 很关键**：保证跨段引用的内容不会丢失。例如“上段介绍了 xxx，这段继续讲…”这种语义就需要重叠部分。

---

### 三、切割之后的步骤

1. **Embedding 向量化**
    
    - 使用模型如：`text-embedding-3-large`（OpenAI）、`bge`, `e5`, `mxbai`, `GTE`, `MiniLM` 等
        
    - 输入每个 chunk，输出一个向量
        
2. **存入向量数据库**
    
    - 常见数据库：FAISS（本地）、Milvus、Pinecone、Qdrant、Weaviate
        
    - 通常存储形式为：
        
        ```json
        {
          "id": "chunk_001",
          "text": "这是一段讲述机器学习基本概念的内容...",
          "embedding": [0.123, 0.234, ..., 0.998],
          "metadata": {
            "source": "ml_book.pdf",
            "page": 12,
            "chunk_index": 5
          }
        }
        ```
        
3. **配置检索器（Retriever）**
    
    - 支持向量相似度检索 + Metadata 过滤
        
    - 可设置 Top-K，或结合 Reranker 提升质量
        

---

### 四、最佳实践建议

|场景|推荐切割策略|
|---|---|
|Chat 文本、短文档|每 300~500 字一段 + 50 字重叠|
|Markdown 文档|按 H2/H3 结构切分|
|技术文档 / API 文档|按函数/标题切分 + 结构化|
|网页 HTML|用 BeautifulSoup 提取文本块|
|多语种混合|采用 token 分割，避免字符长度误差|

---

### 五、可视化示意（切割+入库）

```text
📘 原始文档（技术手册）：
--------------------------
第1章：什么是AI
AI 是人工智能的简称...

第2章：神经网络
神经网络是一种模拟人脑的系统...

→ 切成以下 chunk：
1. "第1章：什么是AI\nAI 是人工智能..."
2. "第2章：神经网络\n神经网络是一种..."
...

→ 每段进行 embedding → 存入向量数据库
```

---

### 六、总结一张表

|阶段|工具/方法|作用|
|---|---|---|
|文本切割|`RecursiveCharacterTextSplitter`、句子分割|控制语义单元，防止 token 爆炸|
|向量化|OpenAI/BGE/E5 等 embedding 模型|将 chunk 转换为向量|
|存储|FAISS、Milvus、Pinecone 等|快速相似度检索|
|检索|`.as_retriever()` 或 RAG pipeline|支持 Top-K 检索、Metadata 过滤|

---



# 5.      对话记忆功能是所有的数据都保存吗，超出最大限度怎么办，prompt会改变吗
这是个很好的问题！关于**对话记忆功能的数据保存范围、超出限制后的处理，以及对 prompt 的影响**，我给你详细讲讲：

---
### 一、对话记忆是否保存所有数据？

- **不一定保存所有历史数据。**
    
- 对话记忆主要关注**对当前对话上下文有用的信息**，包括：
    
    - 最近几轮对话（短期记忆）
        
    - 重要的事实、用户偏好（长期记忆）
        
- 原因：
    
    - 模型上下文窗口（Context Window）大小有限（比如 GPT-4o 128k tokens，但普通 GPT-4 8k 或 32k tokens）
        
    - 传输和计算成本限制
        
    - 信息冗余降低效果，增加模型混乱风险
        

---

### 二、超出最大上下文限制怎么办？

#### 1. **截断（裁剪）**

- 最直接方法是**丢弃最早的对话**，只保留最近的 N 条消息。
    
- 一般使用 **队列（FIFO）** 结构实现，比如保留最近 10~20 轮对话。
    

#### 2. **摘要（Compression）**

- 利用摘要模型（如 GPT 生成摘要）压缩历史对话内容，减少 token 数。
    
- 生成的摘要作为长期记忆片段，补充当前上下文。
    

#### 3. **分层记忆管理**

- 短期记忆用队列保存最近对话；
    
- 长期记忆用向量数据库保存重要事实和知识点，通过相似度召回加入上下文。
    

#### 4. **动态记忆更新**

- 对话过程中，重要信息会被提取并保存到长期记忆库；
    
- 在每次调用模型前，通过检索向量库获得相关历史内容，动态注入 prompt。
    

---

### 三、Prompt 会改变吗？

- **是的，prompt 会根据记忆管理策略动态调整。**
    

#### 1. **简单裁剪的 prompt**

- 只把最新 N 条对话拼成 prompt，prompt 变短；
    
- 早期对话被丢弃，模型只能依赖最近的上下文。
    

#### 2. **摘要增强的 prompt**

- 把对话摘要放在 prompt 前端或特定位置，提供上下文大纲；
    
- 结合最新对话，实现语义连续。
    

#### 3. **检索增强的 prompt**

- 从向量记忆库检索相关知识片段，加在对话前面或后面；
    
- 有点像 RAG，扩展了 prompt 内容，但要注意上下文窗口限制。
    

#### 4. **系统提示（System Prompt）**

- 可能会随记忆状态更新（例如“你是一个记住用户偏好的助手”），
    
- 也可以动态调整以引导模型使用记忆。
    

---

### 四、总结表格

|方面|说明|
|---|---|
|是否保存全部数据|一般不保存所有，重点保存最近上下文和重要信息|
|超出上下文限制|裁剪历史、摘要压缩、长期记忆检索结合|
|Prompt 变化|会动态调整，可能变短、加摘要、插入检索内容|
|记忆存储结构|队列（短期）、向量数据库+键值对（长期）|

---

### 五、举个简单示例

```python
# 伪代码示意记忆管理

MAX_TURNS = 10  # 最多保存10轮对话

class ConversationMemory:
    def __init__(self):
        self.history = []

    def add_turn(self, user_msg, bot_msg):
        self.history.append({"user": user_msg, "bot": bot_msg})
        if len(self.history) > MAX_TURNS:
            self.history.pop(0)  # 丢弃最早对话

    def get_prompt(self):
        # 拼接最近对话形成prompt
        prompt = ""
        for turn in self.history:
            prompt += f"User: {turn['user']}\nAssistant: {turn['bot']}\n"
        return prompt

memory = ConversationMemory()
memory.add_turn("你好", "你好，有什么我可以帮忙的？")
# 超出限制时，会丢弃最早的对话
prompt = memory.get_prompt()
```

---
6.      非阻塞式响应式怎么实现的，引入什么依赖

7.      项目是基于什么协议的，和https什么区别

9.      Zset是redis的什么结构，如何实现的

10.   Lua脚本使用的场景，作用

11.   讲讲乐观锁是什么，锁是什么，在什么场景使用了乐观锁

12.   全局唯一id是如何实现的

13.   压测是否做过

14.   单精度和双精度的区别

15.   Tcp的七层网络模型

手撕环节

1.      Sql查找

2.      多线程打印

3.      单例模式

4.      复原ip地址

算法干货

1.      看过哪些论文？

2.      过拟合和欠拟合的区别，如何解决

3.      优化算法的比较：sgd，adam，rmsprop

4.      梯度消失和梯度爆炸的原因和解决方法

5.      Batch normalization的作用和原理

6.      Transform的结构和工作原理

7.      Self-attention机制的计算过程和复杂度分析

8.      归一化方法的区别，layernorm，batchnorm

9.      常见激活函数的优缺点

10.   大模型训练的并行策略，数据并行，模型并行

11.   Llm的kv缓存机制

12.   大模型推理优化的常见方法

13.   Lora，adapter等参数高效微调的方法原理

14.   Leetcode中等、困难题目

15.   实现transformer中的self-attention，实现简单的语言模型训练过程

16.   大数组topk的多种解法和复杂度分析

17.   设计一个分布式训练框架的关键考虑点

18.   如何优化大模型的推理服务延迟和吞吐量

19.   模型量化部署的实践方案

20.   Pytorch的深度使用经验

21.   Deepseed和megatron的分布式训练框架的了解

22.   Cuda编程和性能优化经验